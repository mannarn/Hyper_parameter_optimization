{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step1: Importing Dataset from source( Data base - Boston House Price)"
      ],
      "metadata": {
        "id": "UrTIQ7d3pVoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np                                                               # selected for finals\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])               # Stack the data to create the feature matrix (X) and target vector (y)\n",
        "target = raw_df.values[1::2, 2]\n",
        "\n",
        "column_names = [\n",
        "    \"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\",\n",
        "    \"PTRATIO\", \"B\", \"LSTAT\"\n",
        "]\n",
        "boston_df = pd.DataFrame(data, columns=column_names)                             # Convert to Pandas DataFrame for easier handling\n",
        "boston_df['MEDV'] = target                                                       # Add target column (MEDV)\n",
        "print(boston_df.head())"
      ],
      "metadata": {
        "id": "U6_6YZR7evCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53203e4-5295-476a-98ec-7db5c8c5568b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
            "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
            "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
            "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
            "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
            "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
            "\n",
            "   PTRATIO       B  LSTAT  MEDV  \n",
            "0     15.3  396.90   4.98  24.0  \n",
            "1     17.8  396.90   9.14  21.6  \n",
            "2     17.8  392.83   4.03  34.7  \n",
            "3     18.7  394.63   2.94  33.4  \n",
            "4     18.7  396.90   5.33  36.2  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step2 : Data Pre-processing"
      ],
      "metadata": {
        "id": "ZQ84Ol8HpjR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(boston_df.isnull().sum())                                                  #checking for empty elements in the dataset\n",
        "\n",
        "print(boston_df.describe())                                                      #Statistical data (minimum,maximum, mean, Standard deviation, etc.. )\n",
        "print(boston_df.head())"
      ],
      "metadata": {
        "id": "V1gyjo_DpPIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1c3a1d-1238-46a9-c78a-da5e24ef1b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRIM       0\n",
            "ZN         0\n",
            "INDUS      0\n",
            "CHAS       0\n",
            "NOX        0\n",
            "RM         0\n",
            "AGE        0\n",
            "DIS        0\n",
            "RAD        0\n",
            "TAX        0\n",
            "PTRATIO    0\n",
            "B          0\n",
            "LSTAT      0\n",
            "MEDV       0\n",
            "dtype: int64\n",
            "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
            "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
            "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
            "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
            "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
            "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
            "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
            "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
            "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
            "\n",
            "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
            "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
            "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
            "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
            "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
            "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
            "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
            "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
            "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
            "\n",
            "            LSTAT        MEDV  \n",
            "count  506.000000  506.000000  \n",
            "mean    12.653063   22.532806  \n",
            "std      7.141062    9.197104  \n",
            "min      1.730000    5.000000  \n",
            "25%      6.950000   17.025000  \n",
            "50%     11.360000   21.200000  \n",
            "75%     16.955000   25.000000  \n",
            "max     37.970000   50.000000  \n",
            "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
            "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
            "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
            "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
            "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
            "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
            "\n",
            "   PTRATIO       B  LSTAT  MEDV  \n",
            "0     15.3  396.90   4.98  24.0  \n",
            "1     17.8  396.90   9.14  21.6  \n",
            "2     17.8  392.83   4.03  34.7  \n",
            "3     18.7  394.63   2.94  33.4  \n",
            "4     18.7  396.90   5.33  36.2  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = boston_df.drop('MEDV', axis=1)                                                         # Define features (X) and target (y)\n",
        "y = boston_df['MEDV']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Split into training and testing set in 80:20 ratio\n"
      ],
      "metadata": {
        "id": "0KWrtcQWpkQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step3: Creating Model( XGBOOST)"
      ],
      "metadata": {
        "id": "7Nrt0wiOszdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "xgboost_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "xgboost_model.fit(X_train, y_train)\n",
        "y_pred = xgboost_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")                                         #Model evaluation using Mean squared Error"
      ],
      "metadata": {
        "id": "j21h8EkEqsw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8deb408a-554d-45c1-dca8-8a9f89683614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 6.909231565384943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step4: Hyper-parameter Tuning"
      ],
      "metadata": {
        "id": "btcSsfZVxUmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "param_grid = {                                                                     # We are performing Hyper-parameter tuning using RandomizedsearchCv algo\n",
        "    'n_estimators': [200, 300, 400],                                               # hyperparameters and their ranges\n",
        "    'max_depth': [3, 5, 8],\n",
        "    'learning_rate': [0.01, 0.1, 0.3],\n",
        "    'subsample': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(                                                # Perform hyperparameter tuning using RandomizedSearchCV\n",
        "    estimator=xgboost_model, param_distributions=param_grid,\n",
        "    n_iter=10, scoring='neg_mean_squared_error', cv=5, verbose=2, random_state=42\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = random_search.best_params_                                            # Get the best parameters\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "\n",
        "best_model = random_search.best_estimator_                                        # Evaluate the model with best parameters\n",
        "y_pred_tuned = best_model.predict(X_test)\n",
        "\n",
        "mse_tuned = mean_squared_error(y_test, y_pred_tuned)                              #MSE after hyper-parameter tuning\n",
        "rmse_tuned = np.sqrt(mse_tuned)\n",
        "print(f\"Tuned Root Mean Squared Error (RMSE): {rmse_tuned}\")\n",
        "print(f\"Tuned Mean Squared Error (MSE): {mse_tuned}\")"
      ],
      "metadata": {
        "id": "4RQnhDUzszfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985f3a50-3575-4214-8f50-677a7992f5e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.2s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.2s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.4s\n",
            "[CV] END learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
            "[CV] END learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
            "[CV] END learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
            "[CV] END learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
            "[CV] END learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
            "[CV] END learning_rate=0.01, max_depth=8, n_estimators=300, subsample=0.8; total time=   0.7s\n",
            "[CV] END learning_rate=0.01, max_depth=8, n_estimators=300, subsample=0.8; total time=   0.7s\n",
            "[CV] END learning_rate=0.01, max_depth=8, n_estimators=300, subsample=0.8; total time=   0.7s\n",
            "[CV] END learning_rate=0.01, max_depth=8, n_estimators=300, subsample=0.8; total time=   0.7s\n",
            "[CV] END learning_rate=0.01, max_depth=8, n_estimators=300, subsample=0.8; total time=   0.7s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END learning_rate=0.01, max_depth=8, n_estimators=200, subsample=0.6; total time=   0.4s\n",
            "[CV] END learning_rate=0.01, max_depth=8, n_estimators=200, subsample=0.6; total time=   0.4s\n",
            "[CV] END learning_rate=0.01, max_depth=8, n_estimators=200, subsample=0.6; total time=   0.4s\n",
            "[CV] END learning_rate=0.01, max_depth=8, n_estimators=200, subsample=0.6; total time=   0.4s\n",
            "[CV] END learning_rate=0.01, max_depth=8, n_estimators=200, subsample=0.6; total time=   0.4s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.2s\n",
            "[CV] END learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.7s\n",
            "[CV] END learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END learning_rate=0.3, max_depth=5, n_estimators=400, subsample=0.8; total time=   0.3s\n",
            "[CV] END learning_rate=0.3, max_depth=5, n_estimators=400, subsample=0.8; total time=   0.3s\n",
            "[CV] END learning_rate=0.3, max_depth=5, n_estimators=400, subsample=0.8; total time=   0.4s\n",
            "[CV] END learning_rate=0.3, max_depth=5, n_estimators=400, subsample=0.8; total time=   0.3s\n",
            "[CV] END learning_rate=0.3, max_depth=5, n_estimators=400, subsample=0.8; total time=   0.3s\n",
            "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.3s\n",
            "[CV] END learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.3s\n",
            "[CV] END learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.3s\n",
            "[CV] END learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.4s\n",
            "[CV] END learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.3s\n",
            "Best Hyperparameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.1}\n",
            "Tuned Root Mean Squared Error (RMSE): 2.277324092088964\n",
            "Tuned Mean Squared Error (MSE): 5.186205020408825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test1 BEST\n",
        "Best Hyperparameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
        "Tuned Root Mean Squared Error (RMSE): 2.190574488963846\n",
        "Tuned Mean Squared Error (MSE): 4.798616591699215"
      ],
      "metadata": {
        "id": "oSaUD1gewFGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST2:\n",
        "Best Hyperparameters: {'subsample': 0.6, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
        "Tuned Root Mean Squared Error (RMSE): 2.5263695955890624\n",
        "Tuned Mean Squared Error (MSE): 6.382543333516843\n"
      ],
      "metadata": {
        "id": "Y_ytDk6BwV_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test3:\n",
        "Best Hyperparameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
        "Tuned Root Mean Squared Error (RMSE): 2.4501927451197707\n",
        "Tuned Mean Squared Error (MSE): 6.003444488237557\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "p7CZoi_RwYiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step5: I created a model with specific hyper parameter from the above analysis"
      ],
      "metadata": {
        "id": "ryg2Lg0Iwl8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "                                                                                #Model with specific h-parameter\n",
        "xgboost_model = xgb.XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.8, colsample_bytree=0.8)\n",
        "xgboost_model.fit(X_train, y_train)\n",
        "y_pred = xgboost_model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)                                         # Evaluate the model performance\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n"
      ],
      "metadata": {
        "id": "i3rWEhL_ulPS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac84358-857e-4ca5-cae3-1c8ac1f976a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 2.5466378278954207\n",
            "Mean Squared Error (MSE): 6.485364226467907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Creating Docker Container for the above program"
      ],
      "metadata": {
        "id": "v8cK-7U3xLpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# These steps doesn't included in Colab process"
      ],
      "metadata": {
        "id": "h947ml3rx20Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FROM python:3.8\n",
        "\n",
        "\n",
        "RUN pip install xgboost scikit-learn google-cloud-storage                        # Install necessary libraries\n",
        "RUN pip install pandas\n",
        "\n",
        "COPY main.py .                                                                  # Copy the training script into the container\n",
        "\n",
        "ENTRYPOINT [\"python\", \"./main.py\"]                                              # Set the command to run your training script\n",
        "\n"
      ],
      "metadata": {
        "id": "0VQHT1yvx_Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Download Docker Desktop and Install it in the procedure\n",
        "*   Procedure PDF link is attached\n",
        "           https://docs.docker.com/get-started/\n",
        "*   Use a Editor with your comfort. I used Visual studio\n",
        "*   Create Docker file and paste the above docker file instructions\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D9f_4xs4yV5D"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lJyJ1Zf7E8rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Navigate to the file path in the terminal\n",
        "2.   and follow the commands\n",
        "       docker build -t <image_name> .\n",
        "       docker run <image_name_like_hpar_model>\n",
        "\n",
        "\n",
        "*   Make sure you included the nessary libraries in the RUN command in the docker file\n",
        "*   I Included xgboost scikit-learn google-cloud-storage , Pandas\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aYKRVtyZ177k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Is setting up Google cloud platform for vertex AI"
      ],
      "metadata": {
        "id": "JrJFDsa6DAfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enable Artifact registary API : It required Billing so process it"
      ],
      "metadata": {
        "id": "JK05jqdPDHkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And More I played with few of the option on how to run hyperparameter tuning in Vertex AI and It doesnt gone as planned. It shows error \"raining pipeline failed with error message: The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_cpus,aiplatform.googleapis.com/custom_model_training_nvidia_v100_gpus\""
      ],
      "metadata": {
        "id": "LM7hm8xD26Yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I played with virtual machines to work with Kubernetes and I Had a hardtime learning this because of other interviews"
      ],
      "metadata": {
        "id": "avgE1fpM3lu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learned alot and Hope I will learn awesome tech in ML and I am a fresher and I posible give me change to learn more with your team"
      ],
      "metadata": {
        "id": "F07_Mbcs4dJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hope I Hear back good news from you . Thank you"
      ],
      "metadata": {
        "id": "2bIUgRLe45iM"
      }
    }
  ]
}